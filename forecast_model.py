# -*- coding: utf-8 -*-
"""

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NeIdCJ4V21VA7INH4mxZmaVUGyr6SCJP

#1. Data Preprocessing
*   Calculate returns and handle missing values
*   Conduct EDA to identify trends, seasonality, and volatility
*   Apply variance-stabilizing transformations (e.g., log scaling)
"""

# ── Cell 10a: Reinstall pmdarima against the current NumPy ──
!pip uninstall -y pmdarima
!pip install --upgrade pmdarima==2.0.3

# After this cell finishes, please:
# 1) Restart the Colab runtime (Runtime → Restart runtime)
# 2) Re-run all cells from Cell 1 onward

!pip install arch yfinance chart-studio cufflinks

# Commented out IPython magic to ensure Python compatibility.
#1. Data Preprocessing
# Download data

import numpy as np
import pandas as pd
import yfinance as yf
import matplotlib.pyplot as plt
from statsmodels.tsa.seasonal import seasonal_decompose
# %matplotlib inline

plt.rcParams["figure.figsize"] = [12, 8]
import yfinance as yf

df = yf.download(
    tickers="^N225",
    start="2010-01-01",
    end="2025-07-02",
    interval="1d",
    auto_adjust=True
)

# Drop rows with no data
df.dropna(axis=0, how='all', inplace=True)

# Rename
df.rename(columns={'Close': 'Adj Close'}, inplace=True)
# ── Cell 3: Calculate Returns & Drop First NaN ──

# 1) Compute daily percent-returns (×100)
df['ret_nikkei'] = df['Adj Close'].pct_change(1) * 100

# 2) Drop the very first row where ret_nikkei is NaN
df = df[df['ret_nikkei'].notnull()]

# (Optional) confirm it worked
print(df['ret_nikkei'].head())

# ── Cell 4: Exploratory Data Analysis (EDA) ──
# 1) Summary stats
print(df['ret_nikkei'].describe())

# 2) Price & returns plots
df['Adj Close'].plot(title='Nikkei 225 Adjusted Close')
plt.show()

df['ret_nikkei'].plot(title='Daily Returns of Nikkei 225')
plt.show()

# 3) Histogram of returns
df['ret_nikkei'].hist(bins=50)
plt.title('Histogram of Nikkei 225 Daily Returns')
plt.show()
# ── Cell 5: Seasonality & Volatility ──
from statsmodels.tsa.seasonal import seasonal_decompose

# decomposition on price (period≈252 trading days)
decomp = seasonal_decompose(df['Adj Close'], model='multiplicative', period=252)
decomp.plot()
plt.suptitle('Seasonal Decomposition of Nikkei 225 Price', y=1.02)
plt.show()

# 30-day rolling volatility of returns
df['volatility_nikkei'] = df['ret_nikkei'].rolling(window=30).std()
df['volatility_nikkei'].plot(title='30-Day Rolling Volatility of Nikkei 225 Returns')
plt.show()
# ── Cell 6: Variance‐Stabilizing Transform ──
# fractional returns for log transform
df['return_frac_nikkei'] = df['Adj Close'].pct_change(1)

# log‐returns
df['log_return_nikkei'] = np.log(df['return_frac_nikkei'] + 1)
df['log_return_nikkei'].plot(title='Log Returns of Nikkei 225')
plt.show()

"""#2. Benchmark Models
*   Implement and evaluate:
  *   Naïve forecast
  *   Naïve forecast with Random Walk
  *   Simple Moving Average (SMA)
  *   Exponentially Weighted Moving Average (EWMA)
*   Use RMSE/MAE for evaluation
"""

#2. Benchmark Models
# ── Cell 7: Train/Test Split & Naïve / Random‐Walk Forecasts ──
from sklearn.metrics import mean_squared_error, mean_absolute_error

# 80/20 split
split_idx = int(len(df)*0.8)
train = df['ret_nikkei'].iloc[:split_idx]
test  = df['ret_nikkei'].iloc[split_idx:]

print(f"Training on {train.index[0].date()} to {train.index[-1].date()},\n"
      f"testing on {test.index[0].date()} to {test.index[-1].date()}")

# 1) Naïve forecast: next-day return = yesterday’s return
naive_pred = test.shift(1)
naive_pred.iloc[0] = train.iloc[-1]   # seed first prediction with last train obs

# 2) Random Walk with drift: constant = mean of historical returns
drift = train.mean()
drift_pred = pd.Series(drift, index=test.index)

# ── Cell 8: SMA, EWMA & Evaluation ──
# 3) Simple Moving Average (5-day)
sma = df['ret_nikkei'].rolling(window=5).mean()
sma_pred = sma.shift(1).loc[test.index]

# 4) Exponentially Weighted Moving Average (span=5)
ewma = df['ret_nikkei'].ewm(span=5, adjust=False).mean()
ewma_pred = ewma.shift(1).loc[test.index]

# Evaluate all methods
methods = {
    'Naïve':     naive_pred,
    'RW_drift':  drift_pred,
    'SMA_5day':  sma_pred,
    'EWMA_5day': ewma_pred
}

results = pd.DataFrame(columns=['RMSE','MAE'])
for name, pred in methods.items():
    rmse = np.sqrt(mean_squared_error(test, pred))
    mae  = mean_absolute_error(test, pred)
    results.loc[name] = [rmse, mae]

print(results)

# ── Cell 9: Plotting Actual vs Forecasted Returns ──
import matplotlib.pyplot as plt

# Recompute train/test and forecasts (if you haven't just run Cells 7–8)
split_idx = int(len(df) * 0.8)
train = df['ret_nikkei'].iloc[:split_idx]
test  = df['ret_nikkei'].iloc[split_idx:]

# 1) Naïve
naive_pred = test.shift(1)
naive_pred.iloc[0] = train.iloc[-1]

# 2) Random Walk with drift
drift = train.mean()
drift_pred = pd.Series(drift, index=test.index)

# 3) SMA (5-day)
sma = df['ret_nikkei'].rolling(window=5).mean()
sma_pred = sma.shift(1).loc[test.index]

# 4) EWMA (span=5)
ewma = df['ret_nikkei'].ewm(span=5, adjust=False).mean()
ewma_pred = ewma.shift(1).loc[test.index]

methods = {
    'Naïve':     naive_pred,
    'RW_drift':  drift_pred,
    'SMA_5day':  sma_pred,
    'EWMA_5day': ewma_pred
}

for name, pred in methods.items():
    plt.figure()
    plt.plot(test.index, test.values, label='Actual')
    plt.plot(pred.index, pred.values, label='Forecast')
    plt.title(f'{name} Forecast vs Actual Returns')
    plt.legend()
    plt.show()

"""#3. Classical Models
*   Holt-Winters: Test additive/multiplicative versions
*   ARIMA: Determine optimal $$(p,d,q)$$ via ACF/PACF
*   ARIMAX/SARIMA(X): Incorporate exogenous variables (e.g., USD/JPY exchange
rate)
"""

#3. Classical Models
# ── Cell 10 (final): Holt–Winters on Price ──
import warnings
from statsmodels.tools.sm_exceptions import ValueWarning
warnings.filterwarnings("ignore", category=ValueWarning)

from statsmodels.tsa.holtwinters import ExponentialSmoothing
import numpy as np
from sklearn.metrics import mean_squared_error, mean_absolute_error

# train/test split
split_idx   = int(len(df) * 0.8)
train_price = df['Adj Close'].iloc[:split_idx]
test_price  = df['Adj Close'].iloc[split_idx:]

# 1) Additive trend only, **undamped** (so slope continues)
hw_add = ExponentialSmoothing(
    train_price,
    trend="add",
    seasonal=None,
    initialization_method="estimated"
).fit()

# 2) Additive trend + additive seasonality
hw_add_seas = ExponentialSmoothing(
    train_price,
    trend="add",
    seasonal="add",
    seasonal_periods=252,
    initialization_method="estimated"
).fit()

# 3) Additive trend + multiplicative seasonality
hw_mul = ExponentialSmoothing(
    train_price,
    trend="add",
    seasonal="mul",
    seasonal_periods=252,
    initialization_method="estimated"
).fit()

# 4) Forecast
hw_add_pred      = hw_add.forecast(len(test_price))
hw_add_seas_pred = hw_add_seas.forecast(len(test_price))
hw_mul_pred      = hw_mul.forecast(len(test_price))

# 5) Evaluate
for name, pred in [
    ("HW_Add (undamped)",      hw_add_pred),
    ("HW_Add + Seasonality",   hw_add_seas_pred),
    ("HW_Mul + Seasonality",   hw_mul_pred)
]:
    rmse = np.sqrt(mean_squared_error(test_price, pred))
    mae  = mean_absolute_error(test_price, pred)
    print(f"{name:20s}  RMSE={rmse:.2f}, MAE={mae:.2f}")




# ── Cell 10b (extended): Holt–Winters Returns & Evaluation ──

import numpy as np
from sklearn.metrics import mean_squared_error, mean_absolute_error

# (Re-)define your test_ret here so it’s always available
split_idx = int(len(df) * 0.8)
test_ret  = df['ret_nikkei'].iloc[split_idx:]

# 1) Align forecasts onto the test_price index (assumes hw_*_pred already computed)
hw_add_pred.index      = test_price.index
hw_add_seas_pred.index = test_price.index
hw_mul_pred.index      = test_price.index

# 2) Compute forecast returns (%)
hw_add_ret      = hw_add_pred.pct_change(1) * 100
hw_add_seas_ret = hw_add_seas_pred.pct_change(1) * 100
hw_mul_ret      = hw_mul_pred.pct_change(1) * 100

# 3) Drop NaNs
hw_add_ret      = hw_add_ret.dropna()
hw_add_seas_ret = hw_add_seas_ret.dropna()
hw_mul_ret      = hw_mul_ret.dropna()

# 4) Evaluate in returns space
for name, pred in [
    ("HW_Add_ret",      hw_add_ret),
    ("HW_Add_seas_ret", hw_add_seas_ret),
    ("HW_Mul_seas_ret", hw_mul_ret)
]:
    obs = test_ret.loc[pred.index]   # now test_ret exists!
    rmse = np.sqrt(mean_squared_error(obs, pred))
    mae  = mean_absolute_error(obs, pred)
    print(f"{name:17s}  Return RMSE={rmse:.4f}%, MAE={mae:.4f}%")






# ── Cell 11 (new): ARIMA Order Selection & Forecast ──
import itertools
import warnings
import numpy as np
from statsmodels.tsa.arima.model import ARIMA
from sklearn.metrics import mean_squared_error, mean_absolute_error

# silence harmless convergence warnings
warnings.filterwarnings("ignore")

# Re-compute train/test on returns
split_idx = int(len(df) * 0.8)
train_ret = df['ret_nikkei'].iloc[:split_idx]
test_ret  = df['ret_nikkei'].iloc[split_idx:]

# Grid‐search p∈0–3, d∈0–2, q∈0–3 for lowest AIC
best_aic, best_order = np.inf, None
for p, d, q in itertools.product(range(4), range(3), range(4)):
    try:
        m = ARIMA(train_ret, order=(p, d, q))
        r = m.fit()
        if r.aic < best_aic:
            best_aic, best_order = r.aic, (p, d, q)
    except:
        continue

print(f"Selected ARIMA order: p={best_order[0]}, d={best_order[1]}, q={best_order[2]} (AIC={best_aic:.1f})")

# Fit & forecast
model = ARIMA(train_ret, order=best_order).fit()
arima_pred = model.forecast(steps=len(test_ret))

# Evaluate
rmse_arima = np.sqrt(mean_squared_error(test_ret, arima_pred))
mae_arima  = mean_absolute_error(test_ret, arima_pred)
print(f"ARIMA Forecast  RMSE={rmse_arima:.4f}, MAE={mae_arima:.4f}")


# ── Cell 12 (new): ARIMAX via SARIMAX ──
from statsmodels.tsa.statespace.sarimax import SARIMAX
import numpy as np
from sklearn.metrics import mean_squared_error, mean_absolute_error

# 1) Prepare USD/JPY returns (as before)
jpy = yf.download(
    tickers="JPY=X",
    start="2010-01-01",
    end="2025-07-02",
    interval="1d",
    auto_adjust=True
)["Close"].pct_change(1) * 100
jpy = jpy.reindex(df.index).fillna(method="ffill")

# Align train/test
exo_train = jpy.iloc[:split_idx]
exo_test  = jpy.iloc[split_idx:]

# 2) Fit SARIMAX with same (p,d,q) and exogenous
mx = SARIMAX(
    train_ret,
    exog=exo_train,
    order=best_order,
    enforce_stationarity=False,
    enforce_invertibility=False
).fit(disp=False)

# 3) Forecast with exog
arimax_pred = mx.get_forecast(steps=len(test_ret), exog=exo_test).predicted_mean

# Evaluate
rmse_arimax = np.sqrt(mean_squared_error(test_ret, arimax_pred))
mae_arimax  = mean_absolute_error(test_ret, arimax_pred)
print(f"ARIMAX Forecast  RMSE={rmse_arimax:.4f}, MAE={mae_arimax:.4f}")

# align forecast series to the test date index
arima_pred.index  = test_ret.index
arimax_pred.index = test_ret.index



# ── Cell 13 (updated): Plot Holt–Winters Variants ──
import matplotlib.pyplot as plt

plt.figure(figsize=(10,6))
plt.plot(test_price.index, test_price,                label='Actual Price', color='black')
plt.plot(hw_add_pred.index, hw_add_pred,              label='HW Add (undamped)',   color='tab:orange')
plt.plot(hw_add_seas_pred.index, hw_add_seas_pred,    label='HW Add + Seasonal',   color='tab:green')
plt.plot(hw_mul_pred.index, hw_mul_pred,              label='HW Mul + Seasonal',   color='tab:red')
plt.title('Holt–Winters Price Forecasts vs Actual Nikkei 225')
plt.xlabel('Date')
plt.ylabel('Index Level')
plt.legend()
plt.show()


# 2) ARIMA & ARIMAX Returns vs Actual
plt.figure(figsize=(10,6))
plt.plot(test_ret.index, test_ret, label='Actual Returns', color='black')
plt.plot(arima_pred.index, arima_pred,
         label=f'ARIMA (RMSE={rmse_arima:.2f}, MAE={mae_arima:.2f})',
         color='tab:orange')
plt.plot(arimax_pred.index, arimax_pred,
         label=f'ARIMAX (RMSE={rmse_arimax:.2f}, MAE={mae_arimax:.2f})',
         color='tab:green')

plt.title('ARIMA vs ARIMAX Forecasts vs Actual Returns')
plt.xlabel('Date')
plt.ylabel('Daily Return (%)')
plt.legend()
plt.show()

"""#4. Advanced Models
*   Prophet (Meta):
  *   Model seasonality and holidays
  *   Tune changepoint parameters
*   N-BEATS:
  *   Implement using PyTorch/TensorFlow
  *   Experiment with stack architectures
  *   Backtest forecasts iteratively
"""

# 1) Ensure your main DataFrame `df` has a DatetimeIndex
#    (you probably did this when downloading)
df.index = pd.to_datetime(df.index)

# 2) Reset the index so “Date” becomes a normal column
df_reset = df.reset_index()

# 3) Select only the two columns Prophet wants: “ds” and “y”
df_prophet = (
    df_reset
    [['Date', 'Adj Close']]             # pick the date & price cols
    .rename(columns={                   # rename for Prophet
        'Date': 'ds',
        'Adj Close': 'y'
    })
)

#4. Advanced Models

# ── Cell 14: Prophet Forecast (COVID Highlight Only) ──
!pip install prophet holidays yfinance --quiet

import pandas as pd
import yfinance as yf
from prophet import Prophet
import matplotlib.pyplot as plt

# 0) Extract Nikkei-225 Adjusted Close as a Series
adj = df['Adj Close']
price_series = adj['^N225'] if isinstance(adj, pd.DataFrame) else adj

# 1) Build df_prophet with just ds & y
df_prophet = price_series.reset_index()
df_prophet.columns = ['ds','y']
df_prophet['ds'] = pd.to_datetime(df_prophet['ds'])
df_prophet['y']  = df_prophet['y'].astype(float)

# 2) USD/JPY daily returns as regressor
jpy = (
    yf.download("JPY=X",
                start=df_prophet.ds.min().strftime('%Y-%m-%d'),
                end=  df_prophet.ds.max().strftime('%Y-%m-%d'),
                interval='1d',
                auto_adjust=True
               )['Close']
    .pct_change(1).mul(100)
    .reindex(df_prophet.ds)
    .fillna(method='ffill')
)
df_prophet['USDJPY'] = jpy.values.astype(float)

# 3) Drop any missing rows
df_prophet.dropna(subset=['y','USDJPY'], inplace=True)

# 4) Fit Prophet with extra changepoint flexibility
m = Prophet(
    yearly_seasonality=True,
    weekly_seasonality=True,
    daily_seasonality=False,
    n_changepoints=50,
    changepoint_prior_scale=0.5,
    changepoint_range=0.95
)
m.add_regressor('USDJPY')
m.add_country_holidays(country_name='JP')

m.fit(df_prophet)
# 5) Forecast next 30 business days
future = m.make_future_dataframe(periods=30, freq='B')
future['USDJPY'] = jpy.reindex(future.ds).fillna(method='ffill').values
forecast = m.predict(future)

# 6) Default Prophet plot + COVID highlight
fig = m.plot(forecast)
ax  = fig.gca()

# Shade COVID Crash (Feb–Apr 2020)
import pandas as pd
ax.axvspan(
    pd.to_datetime("2020-02-15"),
    pd.to_datetime("2020-04-30"),
    color='grey', alpha=0.3, label='COVID Crash'
)

# Tidy up title & legend
ax.set_title("Prophet: Nikkei 225 30-Day Price Forecast (COVID Highlight Only)")
ax.legend(loc='upper left')

plt.show()

# 7) Components plot stays the same
fig2 = m.plot_components(forecast)
plt.show()





# ── Cell 15 (revised): Hold-out Test and Evaluate N-Beats ──
import torch, numpy as np
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sklearn.metrics import mean_squared_error, mean_absolute_error
import matplotlib.pyplot as plt

# Hyperparameters
backcast_length = 60
forecast_length = 30
batch_size      = 32
epochs          = 100
patience        = 10
lr              = 1e-3

# 0) Split your returns series into train/test
series      = df['ret_nikkei']
series_train = series.iloc[:-forecast_length]
series_test  = series.iloc[-forecast_length:]

# 1) Define the sliding-window Dataset as before
class TimeSeriesDataset(Dataset):
    def __init__(self, series, bw, fw):
        self.data = series.values.astype('float32')
        self.bw, self.fw = bw, fw
    def __len__(self):
        return len(self.data) - self.bw - self.fw + 1
    def __getitem__(self, idx):
        x = self.data[idx:idx+self.bw]
        y = self.data[idx+self.bw:idx+self.bw+self.fw]
        return x, y

# 2) Build a training dataset only on the TRAIN slice
train_ds = TimeSeriesDataset(series_train, backcast_length, forecast_length)
train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)

# 3) (Optionally) build a small validation split for early-stopping
n_val    = int(0.1 * len(train_ds))
n_train  = len(train_ds) - n_val
train_sub, val_sub = torch.utils.data.random_split(train_ds, [n_train, n_val])
train_loader = DataLoader(train_sub, batch_size=batch_size, shuffle=True)
val_loader   = DataLoader(val_sub,   batch_size=batch_size)

# 4) Define the N-Beats model exactly as before…
class NBeatsBlock(nn.Module):
    def __init__(self, inp, out, hid=128):
        super().__init__()
        self.fc1 = nn.Linear(inp, hid)
        self.fc2 = nn.Linear(hid, hid)
        self.fc3 = nn.Linear(hid, out)
        self.relu= nn.ReLU()
    def forward(self, x):
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        return self.fc3(x)

class NBeats(nn.Module):
    def __init__(self, bw, fw, hid=128, n_blocks=3):
        super().__init__()
        self.blocks = nn.ModuleList([NBeatsBlock(bw, fw, hid) for _ in range(n_blocks)])
    def forward(self, x):
        forecast = torch.zeros((x.size(0), forecast_length), device=x.device)
        for b in self.blocks:
            forecast = forecast + b(x)
        return forecast

device    = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model     = NBeats(backcast_length, forecast_length).to(device)
criterion = nn.MSELoss()
optim     = torch.optim.Adam(model.parameters(), lr=lr)

# 5) Train with early-stopping on the training slice
best_val = float('inf')
wait     = 0
for ep in range(1, epochs+1):
    model.train()
    for xb, yb in train_loader:
        xb, yb = xb.to(device), yb.to(device)
        optim.zero_grad()
        loss = criterion(model(xb), yb)
        loss.backward()
        optim.step()

    # validate
    model.eval()
    val_loss = 0
    with torch.no_grad():
        for xb, yb in val_loader:
            xb, yb = xb.to(device), yb.to(device)
            val_loss += criterion(model(xb), yb).item()
    val_loss /= len(val_loader)
    if val_loss < best_val:
        best_val, wait = val_loss, 0
    else:
        wait += 1
    if wait >= patience:
        print(f"Early stopping at epoch {ep}")
        break

# 6) Produce a single forecast on the held-out test window
model.eval()
last_window = torch.tensor(
    series_train.values[-backcast_length:],
    dtype=torch.float32
).unsqueeze(0).to(device)

with torch.no_grad():
    pred = model(last_window).cpu().numpy().flatten()

# 7) Evaluate against the true test slice
rmse = np.sqrt(mean_squared_error(series_test, pred))
mae  = mean_absolute_error(series_test, pred)
print(f"N-Beats Test RMSE={rmse:.4f}%, MAE={mae:.4f}%")

# 8) Plot the last 100 points of actual and the 30-day test forecast
plt.figure(figsize=(10,6))
start = series_train.index[-100]
plt.plot(series.loc[start:], label='Recent Returns', alpha=0.6)
plt.plot(series_test.index, pred,     label='N-Beats Forecast', linewidth=2)
plt.title(f'N-Beats: Test Forecast vs Actual (RMSE={rmse:.4f}%, MAE={mae:.4f}%)')
plt.xlabel('Date'); plt.ylabel('Return (%)')
plt.legend(); plt.show()

# Additional Metrics Functions
def mape(y_true, y_pred):
    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100

def mase(y_true, y_pred, naive_forecast):
    mae_naive = mean_absolute_error(y_true, naive_forecast)
    mae_model = mean_absolute_error(y_true, y_pred)
    return mae_model / mae_naive

# Prepare results DataFrame
results = pd.DataFrame(columns=['RMSE', 'MAE', 'MAPE', 'MASE'])

# Naive forecast for MASE denominator, shifted by 1
naive_forecast = test.shift(1)

for name, pred in methods.items():
    aligned_df = pd.concat([test, pred, naive_forecast], axis=1, keys=['actual', 'pred', 'naive'])
    aligned_df.dropna(inplace=True)  # Ensures all series align without NaNs

    rmse = np.sqrt(mean_squared_error(aligned_df['actual'], aligned_df['pred']))
    mae = mean_absolute_error(aligned_df['actual'], aligned_df['pred'])
    mape_val = mape(aligned_df['actual'], aligned_df['pred'])
    mase_val = mase(aligned_df['actual'], aligned_df['pred'], aligned_df['naive'])

    results.loc[name] = [rmse, mae, mape_val, mase_val]

print(results)

df['rolling_mean'] = df['Adj Close'].rolling(window=30).mean()
df['volatility'] = df['ret_nikkei'].rolling(window=30).std()

fig, ax1 = plt.subplots(figsize=(12,6))

ax1.set_title('Nikkei 225 Price with Rolling Mean and Volatility')
ax1.plot(df['Adj Close'], label='Adjusted Close Price', color='blue')
ax1.plot(df['rolling_mean'], label='30-Day Rolling Mean', color='orange')
ax1.set_ylabel('Price')

ax2 = ax1.twinx()
ax2.plot(df['volatility'], label='30-Day Volatility', color='red', alpha=0.4)
ax2.set_ylabel('Volatility (%)')

fig.legend(loc='upper left')
plt.show()

# 1) Ensure df_prophet['ds'] covers pre-2020 to 2025 properly
print(df_prophet['ds'].min(), df_prophet['ds'].max())

# 2) Build future frame extending well beyond COVID period
future = m.make_future_dataframe(periods=365, freq='B')  # Extend ~1 year beyond latest date
future['USDJPY'] = jpy.reindex(future['ds']).fillna(method='ffill').values

# 3) Predict full forecast including historical fit
forecast = m.predict(future)

# 4) Plot only 2020 period for comparison
start_date = pd.to_datetime("2020-01-01")
end_date = pd.to_datetime("2020-06-01")

plt.figure(figsize=(12,6))
plt.plot(df_prophet['ds'], df_prophet['y'], label='Actual Price', alpha=0.6)
plt.plot(forecast['ds'], forecast['yhat'], label='Prophet Forecast', color='orange')
plt.axvspan(pd.to_datetime("2020-02-15"), pd.to_datetime("2020-04-30"), color='grey', alpha=0.3, label='COVID Crash')
plt.xlim(start_date, end_date)
plt.title('Prophet Forecast vs Actual: COVID Period (Extended Horizon)')
plt.xlabel('Date')
plt.ylabel('Nikkei 225 Price')
plt.legend()
plt.show()

"""
## Critical Analysis: Model Performance during Market Shocks

**Prophet Model**  
- Captures yearly and weekly seasonality well.  
- Flexible with changepoints; however, struggled slightly with abrupt volatility like the COVID crash.  

**N-BEATS Model**  
- Deep learning model handled short-term non-linear fluctuations effectively.  
- Superior at capturing noisy return dynamics compared to traditional models.  

**Holt-Winters & ARIMA**  
- Suitable for stable trend periods.  
- Performance degrades during high-volatility events like financial crises.  

**Conclusion**  
- No single model dominates across all regimes.  
- Ensemble or regime-switching approaches could enhance forecast robustness.
"""
